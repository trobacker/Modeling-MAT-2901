---
title: "Categorical Predictors"
author: "Thomas Robacker"
date: "April 15, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(tidyverse)
```

## Introduction

We've looked at regression models that rely on continuous explanatory variables, but it's also possible to use a discrete or categorical explanatory variable, made up of $k$ distinct groups or levels, to model the mean response. We must be able to make the same assumptions noted previously: that observations are all independent of one another and residuals are normally distributed with an equal variance. Let's look at $k=2$ groups (a binary-valued predictor). 

### Binary Variables: $k=2$

Recall that the regression model can be specified as $Y|X=\beta_0+\beta_1X+\epsilon$ for a response variable $Y$ and predictor $X$ with $\epsilon\sim N(0,\sigma^2)$. Now, suppose our predictor variable is categorical with only two possible levels and observations coded either 0 or 1. For this case, the equation still holds true, but the interpretation of the model parameters isn't really one of an "intercept" and "slope" anymore. Instead, it's better to think of them as being something like two intercepts, where $\beta_0$ provides the *baseline* or *reference* value of the response when $X=0$ and $\beta_1$ represents that *additive effect* on the mean response if $X=1$. 

Let's work with the `survey` data again:

```{r}
summary(survey)
```

Observe the `Sex` variable. We see that the sex data column is a factor vector with two levels, `Female` and `Male` and a missing value. We're going to determine whether there is statistical evidence that the height of a student is affected by sex. This means that we're again interested in modeling height as the response variable, but with the categorical sex variable as the predictor. 

To visulaize this, let's make a boxplot:

```{r}
# Remove missing Sex observation
survey <- survey %>% 
  filter(!is.na(Sex))

#Find mean of each group
sex.mean <- survey %>%
  group_by(Sex) %>%
  summarise(mean = mean(Height, na.rm = T))

sex.mean

# Side-by-side Boxplots
ggplot(survey, aes(x = Sex, y = Height, fill = Sex)) + 
  geom_boxplot() + 
  labs(title = "Boxplot of Height by Sex") + 
  geom_point() +
  stat_summary(fun=mean, geom="point", shape = 20, size = 8, color="magenta", fill = "magenta") +
  theme_minimal()
```

We added the means as magenta points on the boxplots for each sex too. Remember,  linear regressions are defined by the mean outcome. The plot indicates, overall, that males tend to be taller than females, but is there statistical evidence of a difference to back up this claim?

### Linear Regression Model of Binary Variables

To answer this with a simple linear regression model, we use `lm()` just as before:

```{r}
survey.fit <- lm(Height ~ Sex, data = survey)
summary(survey.fit)
```

Since the predictor is a factor vector instead of a numeric vector, the reporting of the coefficients is slightly different. 

The estimate of $\beta_0$ is the estimate of the mean height if a student is female. The estimate of $\beta_1$ is reported as `SexMale`, the value of 13.139 is the estimated difference that is imparted upon the mean height of a student if they're male. 

If we look at the corresponding regression equation:
\[
\hat{y}=\hat{\beta_0}+\hat{\beta_1}x=165.687+13.139x
\]
we see that the model has been fitted assuming the variable $x$ is defined as "the individidual is male" (0 for no/false and 1 for yes/true). In other words, the level of "female" for the sex variable is assumed as a reference, and it is the effect of "being male" on mean height that is explicitly estimated. 

The hypothesis test for $\beta_0$ and $\beta_1$ is performed with the same hypotheses as before:
\[
H_0: \beta_j=0\\
H_a: \beta_j\neq 0
\]

Again, the test for $\beta_1$ is generally of the most interest since it's this value that tells us whether there is statistical evidence that the mean response variable is affected by the explanatory variable. 

In this context, we see that with the very small P-value for our `SexMale` coefficient, that there is statistical evidence of a difference in average height between sex. In particular, since "female" is the reference level, the intercept, $\beta_0$, indicates that the average height of females is 165.687 cm and that males are estimated to be 13.139 cm taller on average for a total of `165.687+13.139=178.826` cm.  

### Predictions and CIs from a Binary Categorical Variable

With only two possible values for $x$, prediction is straightforward in this context. When we evaluate the equation, we need only specify whether $\hat{\beta_1}$ be included (if the individual is male) or not (if the individual is female). For example, let's create a factor of five extra observations using the same level names as the original survey data:

```{r}
extra.obs <- factor(c("Female", "Male", "Male", "Male", "Female"))
extra.obs

extra.df <- data.frame(Sex = extra.obs)
```

We can then use `predict` to find the mean heights at those extra values of the predictor:

```{r}
predict(survey.fit, newdata = extra.df, interval = "confidence", level = 0.95)
```

We see from the output that the predictions are different only between the levels of the factor. We are 95 percent confidence the average female height is between about 164 and 167 cm, similarly stated for males.

Here are the 95% confidence intervals for the model parameters:

```{r}
confint(survey.fit)
```

